{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e63e52",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "\n",
    "This notebook retrieves tasks from Notion that match debug tasks in an eval folder, and prepares them for comparison and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from notion_client import Client\n",
    "\n",
    "from inbox_agent.config import settings\n",
    "from inbox_agent.notion import query_pages_filtered, get_block_plain_text, extract_property_value\n",
    "from inbox_agent.pydantic_models import NotionTask, AIUseStatus\n",
    "\n",
    "print(\"âœ“ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CONFIGURATION ============\n",
    "# Control which eval folder to use\n",
    "EVAL_FOLDER_NAME = \"eval_1\"  # Change this to use different eval folders (e.g., \"eval_2\", \"eval_3\")\n",
    "\n",
    "# File names for processed tasks\n",
    "NOTION_TASKS_FILE_NAME = \"notion_tasks.json\"\n",
    "DEBUG_TASKS_FILE_NAME = \"debug_tasks.json\"\n",
    "\n",
    "# Derived paths\n",
    "EVAL_FOLDER_PATH = settings.LOGS_DIR / EVAL_FOLDER_NAME\n",
    "NOTION_TASKS_FILE = EVAL_FOLDER_PATH / NOTION_TASKS_FILE_NAME\n",
    "DEBUG_TASKS_FILE = EVAL_FOLDER_PATH / DEBUG_TASKS_FILE_NAME\n",
    "\n",
    "print(f\"âœ“ Eval folder: {EVAL_FOLDER_PATH}\")\n",
    "print(f\"âœ“ Notion tasks will be saved to: {NOTION_TASKS_FILE}\")\n",
    "print(f\"âœ“ Debug tasks will be saved to: {DEBUG_TASKS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 1: Extract Debug Task Information ============\n",
    "\n",
    "def get_debug_task_files(eval_folder: Path) -> list[Path]:\n",
    "    \"\"\"Get all .md debug task files from eval folder.\"\"\"\n",
    "    return [f for f in eval_folder.glob(\"*.md\") if f.is_file()]\n",
    "\n",
    "def extract_task_model_from_debug_file(file_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extract the Task Model JSON from a debug task markdown file.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with task model data, or None if not found\n",
    "    \"\"\"\n",
    "    content = file_path.read_text(encoding='utf-8')\n",
    "    \n",
    "    # Find Task Model section\n",
    "    match = re.search(r'## Task Model\\s+```json\\s+(.*?)\\s+```', content, re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        task_data = json.loads(match.group(1))\n",
    "        return task_data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Warning: Could not parse Task Model JSON in {file_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_debug_task_titles(eval_folder: Path) -> list[str]:\n",
    "    \"\"\"Extract all task titles from debug task files in eval folder.\"\"\"\n",
    "    debug_files = get_debug_task_files(eval_folder)\n",
    "    titles = []\n",
    "    \n",
    "    for file in debug_files:\n",
    "        task_model = extract_task_model_from_debug_file(file)\n",
    "        if task_model and 'title' in task_model:\n",
    "            titles.append(task_model['title'])\n",
    "    \n",
    "    return titles\n",
    "\n",
    "def load_and_save_debug_tasks(eval_folder: Path, output_file: Path) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load debug tasks from markdown files and save to JSON.\n",
    "    \n",
    "    Args:\n",
    "        eval_folder: Folder containing debug task markdown files\n",
    "        output_file: Path to save the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        List of debug task model dicts\n",
    "    \"\"\"\n",
    "    debug_files = get_debug_task_files(eval_folder)\n",
    "    debug_tasks = []\n",
    "    \n",
    "    for file in debug_files:\n",
    "        task_model = extract_task_model_from_debug_file(file)\n",
    "        if task_model:\n",
    "            debug_tasks.append(task_model)\n",
    "    \n",
    "    # Save as JSON\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(debug_tasks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return debug_tasks\n",
    "\n",
    "# Test: Get debug task titles\n",
    "debug_titles = get_debug_task_titles(EVAL_FOLDER_PATH)\n",
    "print(f\"âœ“ Found {len(debug_titles)} debug tasks\")\n",
    "print(f\"Sample titles: {debug_titles[:3]}\")\n",
    "\n",
    "# Load and save debug tasks to JSON\n",
    "debug_tasks_loaded = load_and_save_debug_tasks(EVAL_FOLDER_PATH, DEBUG_TASKS_FILE)\n",
    "print(f\"âœ“ Loaded and saved {len(debug_tasks_loaded)} debug tasks to {DEBUG_TASKS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf074fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 2: Retrieve Tasks from Notion ============\n",
    "\n",
    "def retrieve_tasks_from_notion(notion_client: Client, task_titles: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve tasks from Notion based on titles using OR filter.\n",
    "    \n",
    "    Args:\n",
    "        notion_client: Notion client instance\n",
    "        task_titles: List of task titles to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of Notion page objects\n",
    "    \"\"\"\n",
    "    if not task_titles:\n",
    "        return []\n",
    "    \n",
    "    # Build OR filter for all titles\n",
    "    filter_dict = {\n",
    "        \"or\": [\n",
    "            {\"property\": \"Name\", \"title\": {\"equals\": title}}\n",
    "            for title in task_titles\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Properties to retrieve (relevant to NotionTask model + AIUsefulness, UseAIStatus)\n",
    "    filter_properties = [\n",
    "        \"Name\",           # title\n",
    "        \"Project\",        # projects (relation)\n",
    "        \"Importance\",     # importance\n",
    "        \"Urgency\",        # urgency\n",
    "        \"Impact Score\",   # impact\n",
    "        \"UseAIStatus\",    # ai_use_status\n",
    "        \"AIUsefulness\"    # additional evaluation property\n",
    "    ]\n",
    "    \n",
    "    # Query Notion\n",
    "    response = query_pages_filtered(\n",
    "        notion_client,\n",
    "        settings.NOTION_TASKS_DATA_SOURCE_ID,\n",
    "        filter_dict=filter_dict,\n",
    "        filter_properties=filter_properties\n",
    "    )\n",
    "    \n",
    "    return response['results']\n",
    "\n",
    "# Initialize Notion client and retrieve tasks\n",
    "notion = Client(auth=settings.NOTION_TOKEN)\n",
    "notion_tasks_raw = retrieve_tasks_from_notion(notion, debug_titles)\n",
    "\n",
    "print(f\"âœ“ Retrieved {len(notion_tasks_raw)} tasks from Notion\")\n",
    "if notion_tasks_raw:\n",
    "    print(f\"Sample task ID: {notion_tasks_raw[0]['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 3: Parse and Save Notion Tasks ============\n",
    "\n",
    "def parse_notion_task(notion_page: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a Notion page object into a simplified task dict.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with task properties extracted from Notion\n",
    "    \"\"\"\n",
    "    props = notion_page.get('properties', {})\n",
    "    \n",
    "    # Extract title\n",
    "    title = get_block_plain_text(notion_page)\n",
    "    \n",
    "    # Extract properties using reusable helper\n",
    "    importance = extract_property_value(props.get('Importance'))\n",
    "    urgency = extract_property_value(props.get('Urgency'))\n",
    "    ai_status = extract_property_value(props.get('UseAIStatus'))\n",
    "    ai_usefulness = extract_property_value(props.get('AIUsefulness'))\n",
    "    impact = extract_property_value(props.get('Impact Score'))\n",
    "    \n",
    "    # Convert string importance/urgency to int if available\n",
    "    importance = int(importance) if importance else None\n",
    "    urgency = int(urgency) if urgency else None\n",
    "    \n",
    "    # Extract project relations\n",
    "    project_relations = props.get('Project', {}).get('relation', [])\n",
    "    project_ids = [rel['id'] for rel in project_relations]\n",
    "    \n",
    "    return {\n",
    "        'id': notion_page['id'],\n",
    "        'title': title,\n",
    "        'importance': importance,\n",
    "        'urgency': urgency,\n",
    "        'impact': impact,\n",
    "        'ai_use_status': ai_status,\n",
    "        'ai_usefulness': ai_usefulness,\n",
    "        'project_ids': project_ids,\n",
    "        'created_time': notion_page.get('created_time'),\n",
    "        'last_edited_time': notion_page.get('last_edited_time')\n",
    "    }\n",
    "\n",
    "def save_notion_tasks(tasks_raw: list[dict], output_file: Path):\n",
    "    \"\"\"Parse and save Notion tasks to JSON file.\"\"\"\n",
    "    parsed_tasks = [parse_notion_task(task) for task in tasks_raw]\n",
    "    \n",
    "    # Save as JSON\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(parsed_tasks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return parsed_tasks\n",
    "\n",
    "# Parse and save\n",
    "notion_tasks_parsed = save_notion_tasks(notion_tasks_raw, NOTION_TASKS_FILE)\n",
    "print(f\"âœ“ Saved {len(notion_tasks_parsed)} parsed Notion tasks to {NOTION_TASKS_FILE}\")\n",
    "print(f\"âœ“ Sample task: {notion_tasks_parsed[0]['title'] if notion_tasks_parsed else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38005dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 4: Load and Pair Tasks ============\n",
    "\n",
    "def load_tasks_from_json(json_file: Path) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generic loader for tasks from JSON file.\n",
    "    Works for both Notion tasks and debug tasks.\n",
    "    \"\"\"\n",
    "    if not json_file.exists():\n",
    "        return []\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_task_pairs(notion_tasks_file: Path, debug_tasks_file: Path) -> list[tuple[dict, dict]]:\n",
    "    \"\"\"\n",
    "    Create pairs of (notion_task, debug_task) for evaluation by matching titles.\n",
    "    \n",
    "    Args:\n",
    "        notion_tasks_file: Path to Notion tasks JSON file\n",
    "        debug_tasks_file: Path to debug tasks JSON file\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples where each tuple is (notion_task_dict, debug_task_model_dict)\n",
    "    \"\"\"\n",
    "    # Load both datasets from JSON files\n",
    "    notion_tasks = load_tasks_from_json(notion_tasks_file)\n",
    "    debug_tasks = load_tasks_from_json(debug_tasks_file)\n",
    "    \n",
    "    # Create dict for O(1) lookup by title\n",
    "    debug_tasks_by_title = {task['title']: task for task in debug_tasks}\n",
    "    \n",
    "    # Create pairs based on matching titles\n",
    "    pairs = []\n",
    "    matched_titles = set()\n",
    "    \n",
    "    for notion_task in notion_tasks:\n",
    "        title = notion_task['title']\n",
    "        if title in debug_tasks_by_title:\n",
    "            pairs.append((notion_task, debug_tasks_by_title[title]))\n",
    "            matched_titles.add(title)\n",
    "    \n",
    "    # Report unmatched tasks\n",
    "    unmatched_debug = set(debug_tasks_by_title.keys()) - matched_titles\n",
    "    unmatched_notion = {t['title'] for t in notion_tasks} - matched_titles\n",
    "    \n",
    "    print(f\"âœ“ Created {len(pairs)} task pairs\")\n",
    "    if unmatched_debug:\n",
    "        print(f\"âš  {len(unmatched_debug)} debug tasks not found in Notion\")\n",
    "        print(f\"  Sample unmatched: {list(unmatched_debug)[:3]}\")\n",
    "    if unmatched_notion:\n",
    "        print(f\"âš  {len(unmatched_notion)} Notion tasks not found in debug\")\n",
    "        print(f\"  Sample unmatched: {list(unmatched_notion)[:3]}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create task pairs\n",
    "task_pairs = create_task_pairs(NOTION_TASKS_FILE, DEBUG_TASKS_FILE)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATION DATASET READY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total pairs: {len(task_pairs)}\")\n",
    "if task_pairs:\n",
    "    print(f\"\\nSample pair:\")\n",
    "    print(f\"  Notion task: {task_pairs[0][0]['title']}\")\n",
    "    print(f\"  Debug task:  {task_pairs[0][1]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ HELPER FUNCTIONS FOR ANALYSIS ============\n",
    "\n",
    "def inspect_pair(pair_index: int = 0):\n",
    "    \"\"\"Display detailed comparison of a task pair.\"\"\"\n",
    "    if pair_index >= len(task_pairs):\n",
    "        print(f\"Error: Only {len(task_pairs)} pairs available\")\n",
    "        return\n",
    "    \n",
    "    notion_task, debug_task = task_pairs[pair_index]\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TASK PAIR #{pair_index}: {notion_task['title']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(\"ðŸ“Š NOTION TASK (Actual):\")\n",
    "    print(f\"  Importance: {notion_task['importance']}\")\n",
    "    print(f\"  Urgency:    {notion_task['urgency']}\")\n",
    "    print(f\"  Impact:     {notion_task['impact']}\")\n",
    "    print(f\"  AI Status:  {notion_task['ai_use_status']}\")\n",
    "    print(f\"  AI Useful:  {notion_task['ai_usefulness']}\")\n",
    "    print(f\"  Projects:   {len(notion_task['project_ids'])} linked\")\n",
    "    \n",
    "    print(\"\\nðŸ” DEBUG TASK (Expected):\")\n",
    "    print(f\"  Importance: {debug_task['importance']}\")\n",
    "    print(f\"  Urgency:    {debug_task['urgency']}\")\n",
    "    print(f\"  Impact:     {debug_task['impact']}\")\n",
    "    print(f\"  AI Status:  {debug_task['ai_use_status']}\")\n",
    "    print(f\"  Projects:   {len(debug_task['projects'])} in model\")\n",
    "    print(f\"  Confidence: {debug_task.get('confidence', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nðŸ“ Original Note:\")\n",
    "    print(f\"  {debug_task['original_note'][:200]}...\")\n",
    "    \n",
    "    if debug_task.get('enrichment'):\n",
    "        print(\"\\nâœ¨ Enrichment:\")\n",
    "        print(f\"  {debug_task['enrichment'][:200]}...\")\n",
    "\n",
    "# Example: Inspect first pair\n",
    "if task_pairs:\n",
    "    inspect_pair(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ QUICK ACCESS TO DATA ============\n",
    "\n",
    "# The main data structure for evaluation:\n",
    "# task_pairs = [(notion_task, debug_task), ...]\n",
    "#\n",
    "# Access pattern:\n",
    "# for notion_task, debug_task in task_pairs:\n",
    "#     # Compare notion_task['importance'] with debug_task['importance']\n",
    "#     # etc.\n",
    "\n",
    "print(\"\\nðŸ“¦ Available data structures:\")\n",
    "print(f\"  task_pairs:           {len(task_pairs)} pairs of (notion_task, debug_task)\")\n",
    "print(f\"  notion_tasks_parsed:  {len(notion_tasks_parsed)} Notion tasks\")\n",
    "print(f\"  debug_tasks_loaded:   {len(debug_tasks_loaded)} debug tasks\")\n",
    "print(f\"  debug_titles:         {len(debug_titles)} debug task titles\")\n",
    "print(f\"\\nðŸ’¾ Files:\")\n",
    "print(f\"  {NOTION_TASKS_FILE}\")\n",
    "print(f\"  {DEBUG_TASKS_FILE}\")\n",
    "print(f\"\\nðŸ”§ Helper functions:\")\n",
    "print(f\"  inspect_pair(index) - Detailed view of a task pair\")\n",
    "print(f\"  load_tasks_from_json(file) - Generic JSON loader for tasks\")\n",
    "print(f\"  get_debug_task_files() - List all debug task markdown files\")\n",
    "print(f\"  create_task_pairs(notion_file, debug_file) - Create pairs from JSON files\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Ready for evaluation! ðŸš€\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5da484",
   "metadata": {},
   "source": [
    "## Analysis & Metrics\n",
    "\n",
    "This section computes evaluation metrics comparing debug tasks (expected) with Notion tasks (actual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4cfaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "print(\"âœ“ Analysis libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ COMPUTE METRICS ============\n",
    "\n",
    "def compute_metrics(pairs: list[tuple[dict, dict]]) -> dict:\n",
    "    \"\"\"\n",
    "    Compute accuracy metrics for each property.\n",
    "    \n",
    "    Args:\n",
    "        pairs: List of (notion_task, debug_task) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Dict with metric summaries\n",
    "    \"\"\"\n",
    "    if not pairs:\n",
    "        return {}\n",
    "    \n",
    "    n = len(pairs)\n",
    "    \n",
    "    # Initialize counters\n",
    "    importance_correct = 0\n",
    "    urgency_correct = 0\n",
    "    ai_status_correct = 0\n",
    "    impact_diffs = []\n",
    "    \n",
    "    for notion_task, debug_task in pairs:\n",
    "        # Importance\n",
    "        if notion_task['importance'] == debug_task['importance']:\n",
    "            importance_correct += 1\n",
    "        \n",
    "        # Urgency\n",
    "        if notion_task['urgency'] == debug_task['urgency']:\n",
    "            urgency_correct += 1\n",
    "        \n",
    "        # AI Status\n",
    "        if notion_task['ai_use_status'] == debug_task['ai_use_status']:\n",
    "            ai_status_correct += 1\n",
    "        \n",
    "        # Impact (MAE)\n",
    "        if notion_task['impact'] is not None and debug_task['impact'] is not None:\n",
    "            diff = abs(notion_task['impact'] - debug_task['impact'])\n",
    "            impact_diffs.append(diff)\n",
    "    \n",
    "    metrics = {\n",
    "        'n_pairs': n,\n",
    "        'importance_accuracy': importance_correct / n,\n",
    "        'urgency_accuracy': urgency_correct / n,\n",
    "        'ai_status_accuracy': ai_status_correct / n,\n",
    "        'impact_mae': np.mean(impact_diffs) if impact_diffs else None,\n",
    "        'impact_rmse': np.sqrt(np.mean([d**2 for d in impact_diffs])) if impact_diffs else None\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute and display metrics\n",
    "metrics = compute_metrics(task_pairs)\n",
    "\n",
    "print(\"ðŸ“Š EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sample size: {metrics['n_pairs']} task pairs\")\n",
    "print()\n",
    "print(f\"Importance Accuracy:  {metrics['importance_accuracy']:.2%}\")\n",
    "print(f\"Urgency Accuracy:     {metrics['urgency_accuracy']:.2%}\")\n",
    "print(f\"AI Status Accuracy:   {metrics['ai_status_accuracy']:.2%}\")\n",
    "print(f\"Impact MAE:           {metrics['impact_mae']:.3f}\" if metrics['impact_mae'] else \"Impact MAE:           N/A\")\n",
    "print(f\"Impact RMSE:          {metrics['impact_rmse']:.3f}\" if metrics['impact_rmse'] else \"Impact RMSE:          N/A\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ DISTRIBUTION PLOTS ============\n",
    "\n",
    "def plot_distributions(pairs: list[tuple[dict, dict]]):\n",
    "    \"\"\"Create distribution plots comparing predicted vs actual for each property.\"\"\"\n",
    "    \n",
    "    # Extract data\n",
    "    notion_importance = [n['importance'] for n, d in pairs if n['importance'] is not None]\n",
    "    debug_importance = [d['importance'] for n, d in pairs if d['importance'] is not None]\n",
    "    \n",
    "    notion_urgency = [n['urgency'] for n, d in pairs if n['urgency'] is not None]\n",
    "    debug_urgency = [d['urgency'] for n, d in pairs if d['urgency'] is not None]\n",
    "    \n",
    "    notion_ai_status = [n['ai_use_status'] for n, d in pairs if n['ai_use_status'] is not None]\n",
    "    debug_ai_status = [d['ai_use_status'] for n, d in pairs if d['ai_use_status'] is not None]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle('Property Distributions: Notion (Actual) vs Debug (Expected)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Importance distribution\n",
    "    if notion_importance and debug_importance:\n",
    "        counts_n = Counter(notion_importance)\n",
    "        counts_d = Counter(debug_importance)\n",
    "        labels = sorted(set(notion_importance + debug_importance))\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, [counts_n.get(l, 0) for l in labels], width, label='Notion', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, [counts_d.get(l, 0) for l in labels], width, label='Debug', alpha=0.8)\n",
    "        axes[0].set_xlabel('Importance Value')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].set_title('Importance')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(labels)\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Urgency distribution\n",
    "    if notion_urgency and debug_urgency:\n",
    "        counts_n = Counter(notion_urgency)\n",
    "        counts_d = Counter(debug_urgency)\n",
    "        labels = sorted(set(notion_urgency + debug_urgency))\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1].bar(x - width/2, [counts_n.get(l, 0) for l in labels], width, label='Notion', alpha=0.8)\n",
    "        axes[1].bar(x + width/2, [counts_d.get(l, 0) for l in labels], width, label='Debug', alpha=0.8)\n",
    "        axes[1].set_xlabel('Urgency Value')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title('Urgency')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(labels)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # AI Status distribution\n",
    "    if notion_ai_status and debug_ai_status:\n",
    "        counts_n = Counter(notion_ai_status)\n",
    "        counts_d = Counter(debug_ai_status)\n",
    "        labels = sorted(set(notion_ai_status + debug_ai_status))\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[2].bar(x - width/2, [counts_n.get(l, 0) for l in labels], width, label='Notion', alpha=0.8)\n",
    "        axes[2].bar(x + width/2, [counts_d.get(l, 0) for l in labels], width, label='Debug', alpha=0.8)\n",
    "        axes[2].set_xlabel('AI Use Status')\n",
    "        axes[2].set_ylabel('Count')\n",
    "        axes[2].set_title('AI Use Status')\n",
    "        axes[2].set_xticks(x)\n",
    "        axes[2].set_xticklabels([str(l) for l in labels], rotation=15, ha='right')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create distribution plots\n",
    "plot_distributions(task_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8605df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NotionInboxAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
