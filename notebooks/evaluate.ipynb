{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e63e52",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "\n",
    "This notebook retrieves tasks from Notion that match debug tasks in an eval folder, and prepares them for comparison and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from notion_client import Client\n",
    "\n",
    "from inbox_agent.config import settings\n",
    "from inbox_agent.notion import query_pages_filtered, get_block_plain_text, extract_property_value\n",
    "from inbox_agent.pydantic_models import NotionTask, AIUseStatus\n",
    "\n",
    "print(\"âœ“ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ CONFIGURATION ============\n",
    "# Control which eval folder to use\n",
    "EVAL_FOLDER_NAME = \"eval_1\"  # Change this to use different eval folders (e.g., \"eval_2\", \"eval_3\")\n",
    "\n",
    "# Derived paths\n",
    "EVAL_FOLDER_PATH = settings.LOGS_DIR / EVAL_FOLDER_NAME\n",
    "NOTION_TASKS_FILE = EVAL_FOLDER_PATH / \"notion_tasks.json\"\n",
    "\n",
    "print(f\"âœ“ Eval folder: {EVAL_FOLDER_PATH}\")\n",
    "print(f\"âœ“ Notion tasks will be saved to: {NOTION_TASKS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 1: Extract Debug Task Information ============\n",
    "\n",
    "def get_debug_task_files(eval_folder: Path) -> list[Path]:\n",
    "    \"\"\"Get all .md debug task files from eval folder.\"\"\"\n",
    "    return [f for f in eval_folder.glob(\"*.md\") if f.is_file()]\n",
    "\n",
    "def extract_task_model_from_debug_file(file_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extract the Task Model JSON from a debug task markdown file.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with task model data, or None if not found\n",
    "    \"\"\"\n",
    "    content = file_path.read_text(encoding='utf-8')\n",
    "    \n",
    "    # Find Task Model section\n",
    "    match = re.search(r'## Task Model\\s+```json\\s+(.*?)\\s+```', content, re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        task_data = json.loads(match.group(1))\n",
    "        return task_data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Warning: Could not parse Task Model JSON in {file_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_debug_task_titles(eval_folder: Path) -> list[str]:\n",
    "    \"\"\"Extract all task titles from debug task files in eval folder.\"\"\"\n",
    "    debug_files = get_debug_task_files(eval_folder)\n",
    "    titles = []\n",
    "    \n",
    "    for file in debug_files:\n",
    "        task_model = extract_task_model_from_debug_file(file)\n",
    "        if task_model and 'title' in task_model:\n",
    "            titles.append(task_model['title'])\n",
    "    \n",
    "    return titles\n",
    "\n",
    "# Test: Get debug task titles\n",
    "debug_titles = get_debug_task_titles(EVAL_FOLDER_PATH)\n",
    "print(f\"âœ“ Found {len(debug_titles)} debug tasks\")\n",
    "print(f\"Sample titles: {debug_titles[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf074fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 2: Retrieve Tasks from Notion ============\n",
    "\n",
    "def retrieve_tasks_from_notion(notion_client: Client, task_titles: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve tasks from Notion based on titles using OR filter.\n",
    "    \n",
    "    Args:\n",
    "        notion_client: Notion client instance\n",
    "        task_titles: List of task titles to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of Notion page objects\n",
    "    \"\"\"\n",
    "    if not task_titles:\n",
    "        return []\n",
    "    \n",
    "    # Build OR filter for all titles\n",
    "    filter_dict = {\n",
    "        \"or\": [\n",
    "            {\"property\": \"Name\", \"title\": {\"equals\": title}}\n",
    "            for title in task_titles\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Properties to retrieve (relevant to NotionTask model + AIUsefulness, UseAIStatus)\n",
    "    filter_properties = [\n",
    "        \"Name\",           # title\n",
    "        \"Project\",        # projects (relation)\n",
    "        \"Importance\",     # importance\n",
    "        \"Urgency\",        # urgency\n",
    "        \"Impact Score\",   # impact\n",
    "        \"UseAIStatus\",    # ai_use_status\n",
    "        \"AIUsefulness\"    # additional evaluation property\n",
    "    ]\n",
    "    \n",
    "    # Query Notion\n",
    "    response = query_pages_filtered(\n",
    "        notion_client,\n",
    "        settings.NOTION_TASKS_DATA_SOURCE_ID,\n",
    "        filter_dict=filter_dict,\n",
    "        filter_properties=filter_properties\n",
    "    )\n",
    "    \n",
    "    return response['results']\n",
    "\n",
    "# Initialize Notion client and retrieve tasks\n",
    "notion = Client(auth=settings.NOTION_TOKEN)\n",
    "notion_tasks_raw = retrieve_tasks_from_notion(notion, debug_titles)\n",
    "\n",
    "print(f\"âœ“ Retrieved {len(notion_tasks_raw)} tasks from Notion\")\n",
    "if notion_tasks_raw:\n",
    "    print(f\"Sample task ID: {notion_tasks_raw[0]['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 3: Parse and Save Notion Tasks ============\n",
    "\n",
    "def parse_notion_task(notion_page: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a Notion page object into a simplified task dict.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with task properties extracted from Notion\n",
    "    \"\"\"\n",
    "    props = notion_page.get('properties', {})\n",
    "    \n",
    "    # Extract title\n",
    "    title = get_block_plain_text(notion_page)\n",
    "    \n",
    "    # Extract properties\n",
    "    importance_select = props.get('Importance', {}).get('select')\n",
    "    urgency_select = props.get('Urgency', {}).get('select')\n",
    "    ai_status_select = props.get('UseAIStatus', {}).get('select')\n",
    "    ai_usefulness_select = props.get('AIUsefulness', {}).get('select')\n",
    "    \n",
    "    # Extract project relations\n",
    "    project_relations = props.get('Project', {}).get('relation', [])\n",
    "    project_ids = [rel['id'] for rel in project_relations]\n",
    "    \n",
    "    return {\n",
    "        'id': notion_page['id'],\n",
    "        'title': title,\n",
    "        'importance': int(importance_select['name']) if importance_select else None,\n",
    "        'urgency': int(urgency_select['name']) if urgency_select else None,\n",
    "        'impact': props.get('Impact Score', {}).get('number'),\n",
    "        'ai_use_status': ai_status_select['name'] if ai_status_select else None,\n",
    "        'ai_usefulness': ai_usefulness_select['name'] if ai_usefulness_select else None,\n",
    "        'project_ids': project_ids,\n",
    "        'created_time': notion_page.get('created_time'),\n",
    "        'last_edited_time': notion_page.get('last_edited_time')\n",
    "    }\n",
    "\n",
    "def save_notion_tasks(tasks_raw: list[dict], output_file: Path):\n",
    "    \"\"\"Parse and save Notion tasks to JSON file.\"\"\"\n",
    "    parsed_tasks = [parse_notion_task(task) for task in tasks_raw]\n",
    "    \n",
    "    # Save as JSON\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(parsed_tasks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return parsed_tasks\n",
    "\n",
    "# Parse and save\n",
    "notion_tasks_parsed = save_notion_tasks(notion_tasks_raw, NOTION_TASKS_FILE)\n",
    "print(f\"âœ“ Saved {len(notion_tasks_parsed)} parsed Notion tasks to {NOTION_TASKS_FILE}\")\n",
    "print(f\"âœ“ Sample task: {notion_tasks_parsed[0]['title'] if notion_tasks_parsed else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38005dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ STEP 4: Load and Pair Tasks ============\n",
    "\n",
    "def load_notion_tasks(json_file: Path) -> list[dict]:\n",
    "    \"\"\"Load parsed Notion tasks from JSON file.\"\"\"\n",
    "    if not json_file.exists():\n",
    "        return []\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_debug_tasks_with_models(eval_folder: Path) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Load all debug tasks from eval folder with their task models extracted.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping task title to task model dict\n",
    "    \"\"\"\n",
    "    debug_files = get_debug_task_files(eval_folder)\n",
    "    tasks_by_title = {}\n",
    "    \n",
    "    for file in debug_files:\n",
    "        task_model = extract_task_model_from_debug_file(file)\n",
    "        if task_model and 'title' in task_model:\n",
    "            title = task_model['title']\n",
    "            tasks_by_title[title] = task_model\n",
    "    \n",
    "    return tasks_by_title\n",
    "\n",
    "def create_task_pairs(eval_folder: Path, notion_tasks_file: Path) -> list[tuple[dict, dict]]:\n",
    "    \"\"\"\n",
    "    Create pairs of (notion_task, debug_task) for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples where each tuple is (notion_task_dict, debug_task_model_dict)\n",
    "    \"\"\"\n",
    "    # Load both datasets\n",
    "    notion_tasks = load_notion_tasks(notion_tasks_file)\n",
    "    debug_tasks = load_debug_tasks_with_models(eval_folder)\n",
    "    \n",
    "    # Create pairs based on matching titles\n",
    "    pairs = []\n",
    "    matched_titles = set()\n",
    "    \n",
    "    for notion_task in notion_tasks:\n",
    "        title = notion_task['title']\n",
    "        if title in debug_tasks:\n",
    "            pairs.append((notion_task, debug_tasks[title]))\n",
    "            matched_titles.add(title)\n",
    "    \n",
    "    # Report unmatched tasks\n",
    "    unmatched_debug = set(debug_tasks.keys()) - matched_titles\n",
    "    unmatched_notion = {t['title'] for t in notion_tasks} - matched_titles\n",
    "    \n",
    "    print(f\"âœ“ Created {len(pairs)} task pairs\")\n",
    "    if unmatched_debug:\n",
    "        print(f\"âš  {len(unmatched_debug)} debug tasks not found in Notion\")\n",
    "        print(f\"  Sample unmatched: {list(unmatched_debug)[:3]}\")\n",
    "    if unmatched_notion:\n",
    "        print(f\"âš  {len(unmatched_notion)} Notion tasks not found in debug\")\n",
    "        print(f\"  Sample unmatched: {list(unmatched_notion)[:3]}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create task pairs\n",
    "task_pairs = create_task_pairs(EVAL_FOLDER_PATH, NOTION_TASKS_FILE)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATION DATASET READY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total pairs: {len(task_pairs)}\")\n",
    "if task_pairs:\n",
    "    print(f\"\\nSample pair:\")\n",
    "    print(f\"  Notion task: {task_pairs[0][0]['title']}\")\n",
    "    print(f\"  Debug task:  {task_pairs[0][1]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ HELPER FUNCTIONS FOR ANALYSIS ============\n",
    "\n",
    "def inspect_pair(pair_index: int = 0):\n",
    "    \"\"\"Display detailed comparison of a task pair.\"\"\"\n",
    "    if pair_index >= len(task_pairs):\n",
    "        print(f\"Error: Only {len(task_pairs)} pairs available\")\n",
    "        return\n",
    "    \n",
    "    notion_task, debug_task = task_pairs[pair_index]\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TASK PAIR #{pair_index}: {notion_task['title']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(\"ğŸ“Š NOTION TASK (Actual):\")\n",
    "    print(f\"  Importance: {notion_task['importance']}\")\n",
    "    print(f\"  Urgency:    {notion_task['urgency']}\")\n",
    "    print(f\"  Impact:     {notion_task['impact']}\")\n",
    "    print(f\"  AI Status:  {notion_task['ai_use_status']}\")\n",
    "    print(f\"  AI Useful:  {notion_task['ai_usefulness']}\")\n",
    "    print(f\"  Projects:   {len(notion_task['project_ids'])} linked\")\n",
    "    \n",
    "    print(\"\\nğŸ” DEBUG TASK (Expected):\")\n",
    "    print(f\"  Importance: {debug_task['importance']}\")\n",
    "    print(f\"  Urgency:    {debug_task['urgency']}\")\n",
    "    print(f\"  Impact:     {debug_task['impact']}\")\n",
    "    print(f\"  AI Status:  {debug_task['ai_use_status']}\")\n",
    "    print(f\"  Projects:   {len(debug_task['projects'])} in model\")\n",
    "    print(f\"  Confidence: {debug_task.get('confidence', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Original Note:\")\n",
    "    print(f\"  {debug_task['original_note'][:200]}...\")\n",
    "    \n",
    "    if debug_task.get('enrichment'):\n",
    "        print(\"\\nâœ¨ Enrichment:\")\n",
    "        print(f\"  {debug_task['enrichment'][:200]}...\")\n",
    "\n",
    "# Example: Inspect first pair\n",
    "if task_pairs:\n",
    "    inspect_pair(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ QUICK ACCESS TO DATA ============\n",
    "\n",
    "# The main data structure for evaluation:\n",
    "# task_pairs = [(notion_task, debug_task), ...]\n",
    "#\n",
    "# Access pattern:\n",
    "# for notion_task, debug_task in task_pairs:\n",
    "#     # Compare notion_task['importance'] with debug_task['importance']\n",
    "#     # etc.\n",
    "\n",
    "print(\"\\nğŸ“¦ Available data structures:\")\n",
    "print(f\"  task_pairs:           {len(task_pairs)} pairs of (notion_task, debug_task)\")\n",
    "print(f\"  notion_tasks_parsed:  {len(notion_tasks_parsed)} Notion tasks\")\n",
    "print(f\"  debug_titles:         {len(debug_titles)} debug task titles\")\n",
    "print(f\"\\nğŸ’¾ Files:\")\n",
    "print(f\"  {NOTION_TASKS_FILE}\")\n",
    "print(f\"\\nğŸ”§ Helper functions:\")\n",
    "print(f\"  inspect_pair(index) - Detailed view of a task pair\")\n",
    "print(f\"  get_debug_task_files() - List all debug task markdown files\")\n",
    "print(f\"  load_notion_tasks() - Reload Notion tasks from JSON\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Ready for evaluation! ğŸš€\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
